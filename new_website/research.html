<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KOENIG</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <script type="text/javascript" src="scripts.js"></script>
    <div class="container">
        <div class="sidebar">
            <div class="row" id="home">
                <div class="dot"></div>
            </div>
            <div class="row">2023</div>
            <div class="row">2023</div>
            <div class="row">2022</div>
            <div class="row">2022</div>
            <div class="row">2021</div>
            <div class="row">2020</div>
            <div class="row">2020</div>
            <div class="row">2020</div>
            <div class="row">2019</div>
            <div class="row">2019</div>
            <div class="row">2019</div>
            <div class="row">2019</div>
            <div class="row">2018</div>
        </div>
        <div class="vertical-line"></div>
        <div class="main-content">
            <div class="header">
                <a href="audio.html">AUDIO</a>
                <a href="visual.html">VISUAL</a>
                <a href="research.html">RESEARCH</a>
                <a href="about.html">ABOUT</a>
            </div>
            <div class="content">
                <div class="row">
                    <div>
                        <img src="../images/research/stego.png" alt="Projekt 13">
                    </div>
                    <span>
                        At <a class="link" href="http://merantix-momentum.com">Merantix Momentum</a>, I work as a
                        project lead for our contributions towards the <a class="link"
                            href="https://safetrain-projekt.de/en/">safe.trAIn research project</a> spearheaded by
                        Siemens. Within the project, I investigate reliable and label-efficient
                        computer vision algorithms. We published a follow-up study on STEGO, a self-supervised semantic
                        segmentation method, in the <a class="link" href="https://sites.google.com/view/saiad2023">SAIAD
                            Workshop</a> at <a class="link" href="https://cvpr.thecvf.com/">CVPR 2023</a> in Vancouver.
                        &mdash; with <a class="link" href="https://maxschambach.github.io/">Dr. Schambach</a> and <a
                            class="link" href="https://jotterbach.github.io/">Dr. Otterbach</a>.
                        <div class="paper-resources">
                            <a class="resource-link" href="https://arxiv.org/abs/2304.07314" target="blank">CVPR
                                Paper</a>
                            <a class="resource-link" href="https://github.com/merantix-momentum/stego-studies"
                                target="blank">Code</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=vKadn3MF5BY"
                                target="blank">Talk</a>
                            <a class="resource-link" href="../downloads/research/cvpr23_slides.pdf"
                                target="blank">Slides</a>
                            <a class="resource-link" href="../downloads/research/cvpr23_poster.pdf"
                                target="blank">Poster</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/insights.png" alt="Projekt 12">
                    </div>
                    <span>
                        The <a class="link" href="https://medium.com/merantix-labs-insights">Merantix Momentum Research
                            Insights</a> blog post series is a platform for our
                        researchers to disseminate knowledge, advertising their own research results or meta-analyses
                        like literature reviews. I've published two blog posts, one on
                        <a class="link"
                            href="https://medium.com/merantix-labs-insights/unsupervised-monocular-depth-estimation-2487e09bd1ce">self-supervised
                            depth estimation</a> and one on <a class="link"
                            href="https://medium.com/merantix-labs-insights/label-efficient-semantic-segmentation-f6f601ff0a85">label-efficient
                            semantic segmentation</a>. &mdash; with <a class="link"
                            href="https://jotterbach.github.io/">Dr. Otterbach</a>.
                        <div class="paper-resources">
                            <a class="resource-link"
                                href="https://medium.com/merantix-labs-insights/unsupervised-monocular-depth-estimation-2487e09bd1ce"
                                target="blank">Monocular Depth Post</a>
                            <a class="resource-link"
                                href="https://medium.com/merantix-labs-insights/label-efficient-semantic-segmentation-f6f601ff0a85"
                                target="blank">Semantic Segmentation Post</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/prg.png" alt="Projekt 11">
                    </div>
                    <span>
                        I am the host of the monthly public paper discussion group at the <a class="link"
                            href="https://www.aicampus.berlin/">AI Campus Berlin</a>. With
                        this event, we hold space for the Berlin ML community, scientific exchange, and education. We
                        host a colorful mix of speakers, ranging from in-house engineers and researchers to external
                        speakers. <a class="link" href="mailto:awckoenig@gmail.com">Hit me up</a> if you're interested
                        in speaking.
                        &mdash; with <a class="link" href="https://jotterbach.github.io/">Dr. Otterbach</a>.
                        <div class="paper-resources">
                            <a class="resource-link" href="https://www.meetup.com/ai-campus-berlin/" target="blank">AI
                                Campus Meetup Group</a>
                            <a class="resource-link"
                                href="https://www.aicampus.berlin/event/paper-reading-group-neurips-highlights-2022"
                                target="blank">AI Campus Website</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=z2Tc10zQSjQ"
                                target="blank">Recordings on YouTube</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/squirrel.png" alt="Projekt 10">
                    </div>
                    <span>
                        At <a class="link" href="http://merantix-momentum.com">Merantix Momentum</a>,
                        I'm benchmarking our data loading tool <a class="link"
                            href="https://hub.docker.com/repository/docker/axkoenig/reflex_stack">Squirrel</a>.
                        Preliminary results show that data loading pipelines can benefit greatly from Squirrel's
                        improved throughput. Upon open-sourcing Squirrel, our VP of Engineering, Dr. Thomas
                        Wollmann, showcased some of our initial results at the <a class="link"
                            href="https://www.youtube.com/watch?v=pZPbi4EmqEo">PyData 2022</a>
                        conference in Berlin. I'm also an active code contributor to Squirrel. If you have feedback,
                        <a class="link" href="mailto:awckoenig@gmail.com">let's chat</a>! &mdash;
                        with <a class="link"
                            href="https://scholar.google.com/citations?user=H8tlV-oAAAAJ&hl=de&oi=ao">Dr. Wollmann</a>.
                        <div class="paper-resources">
                            <a class="resource-link" href="https://www.youtube.com/watch?v=pZPbi4EmqEo"
                                target="blank">PyData Talk</a>
                            <a class="resource-link" href="https://github.com/merantix-momentum/squirrel-core"
                                target="blank">Code</a>
                            <a class="resource-link" href="https://zenodo.org/record/7158000#.Y1a1CexBzwM"
                                target="blank">Zenodo</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=Z7ziAp0mb-o"
                                target="blank">Siemens Talk</a>
                            <a class="resource-link" href="../downloads/research/squirrel_slides.pdf"
                                target="blank">Slides</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/grasp_refinement.png" alt="Projekt 09">
                    </div>
                    <span>
                        For my master thesis in the <a class="link" href="http://biorobotics.harvard.edu">Harvard
                            Biorobotics Lab</a>, I investigated the role of tactile sensing in learning and
                        deploying robotic grasping controllers. I'm grateful that this research effort was sponsored
                        through a full scholarship by the German Academic Exchange Service (DAAD) and by funding of
                        the US National Science Foundation (NSF). Our work first got published in the
                        <a class="link" href="https://sites.google.com/view/icra2022-contactrich/">Workshop on
                            Reinforcement Learning for Contact-Rich Manipulation</a>
                        at
                        <a class="link" href="https://www.icra2022.org">ICRA 2022</a>. An extended version was later
                        accepted into <a class="link" href="https://iros2022.org/">IROS 2022</a> as a full paper.
                        &mdash; with <a class="link"
                            href="https://hst.mit.edu/faculty-research/faculty/howe-robert">Prof. Howe</a>, <a
                            class="link" href="http://lucasjanson.fas.harvard.edu">Prof. Janson</a>, and
                        <a class="link" href="http://home.in.tum.de/~menze/">Prof. Menze</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="https://arxiv.org/abs/2109.11234" target="blank">IROS
                                Paper</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=ko4iZgjomvY"
                                target="blank">IROS Talk</a>
                            <a class="resource-link" href="https://openreview.net/forum?id=OqmWRIsvA4O"
                                target="blank">ICRA Paper</a>
                            <a class="resource-link" href="../downloads/research/icra22_workshop_poster.pdf"
                                target="blank">ICRA Poster</a>
                            <a class="resource-link" href="../downloads/research/mt_compressed.pdf"
                                target="blank">thesis</a>
                            <a class="resource-link" href="https://github.com/axkoenig/grasp_refinement"
                                target="blank">code</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=WKhmOKPEYPc"
                                target="blank">demo</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/hand.png" alt="Projekt 08">
                    </div>
                    <span>
                        In preparation for my research project on robotic grasp refinement, I created a simulator for
                        a robotic hand using Gazebo, C++, and the Robot Operating System (ROS). This open-source
                        simulation stack also calculates various metrics that are useful for grasp analysis. The
                        package is available as a pre-built <a class="link"
                            href="https://hub.docker.com/repository/docker/axkoenig/reflex_stack">Docker container</a>.
                        &mdash;
                        with <a class="link" href="https://hst.mit.edu/faculty-research/faculty/howe-robert">Prof.
                            Howe</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="https://github.com/axkoenig/reflex_stack"
                                target="blank">code</a>
                            <a class="resource-link"
                                href="https://hub.docker.com/repository/docker/axkoenig/reflex_stack"
                                target="blank">docker</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/dl4mi.png" alt="Projekt 07">
                    </div>
                    <span>
                        I worked on COVID-19 detection from chest radiographs as part of the Deep
                        Learning in Medical Imaging course at the <a class="link"
                            href="https://en-engineering.tau.ac.il/biomed">Department of Biomedical Engineering</a>
                        at Tel Aviv University. We compare three approaches: (1) transfer learning with a
                        pre-trained network, (2) anomaly detection using an autoencoder trained on healthy lung
                        images, and (3) multi-task learning of image classification and reconstruction.
                        &mdash; with
                        <a class="link" href="http://www.eng.tau.ac.il/~hayit/">Prof. Greenspan</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="../downloads/research/dl4mi_report_compressed.pdf"
                                target="blank">report</a>
                            <a class="resource-link" href="../downloads/research/dl4mi_slides_compressed.pdf"
                                target="blank">slides</a>
                            <a class="resource-link" href="https://github.com/axkoenig/dl4mi" target="blank">code</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/ml4cg.png" alt="Projekt 06">
                    </div>
                    <span>
                        I took part in the Machine Learning in Computer Graphics practical offered by the
                        <a class="link" href="https://en-exact-sciences.tau.ac.il/computer">Blavatnik School of
                            Computer Science</a> at Tel Aviv University. In a small team, we developed a new
                        method for self-supervised class and content disentanglement.
                        &mdash; with <a class="link" href="https://www.cs.tau.ac.il/~dcor/">Prof. Cohen-Or</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="../downloads/research/ml4cg_report_compressed.pdf"
                                target="blank">report</a>
                            <a class="resource-link" href="../downloads/research/ml4cg_slides_compressed.pdf"
                                target="blank">slides</a>
                            <a class="resource-link" href="https://github.com/axkoenig/ml4cg" target="blank">code</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/neuro.png" alt="Projekt 05">
                    </div>
                    <span>
                        I participated in a course on neuroprosthetics at the TUM <a class="link"
                            href="https://www.ei.tum.de/en/bai/home/">Chair for Bio-inspired Information Processing</a>.
                        In the practical part, I coded up the infamous Hodgkin-Huxley model and simulated neuronal
                        behavior with different electrical stimuli. Further, I implemented basic encoding strategies for
                        cochlear implants and a noise vocoder to study the signals as perceived by the patient.
                        &mdash; with
                        <a class="link" href="https://www.professoren.tum.de/en/hemmert-werner">Prof. Hemmert</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="../downloads/research/neuro_compressed.pdf"
                                target="blank">reports</a>
                            <a class="resource-link" href="https://github.com/axkoenig/neuroprosthetics"
                                target="blank">code</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/ipraktikum.png" alt="Projekt 04">
                    </div>
                    <span>
                        In my master's, I took part in TUM's software-engineering practical <a class="link"
                            href="https://ase.in.tum.de/lehrstuhl_1/people/people-archive/134-teaching/winterterm-2019-2020/1063-ipraktikum-ws1920">iPraktikum</a>.
                        Our team built the community-based iPhone application LocalHero for the German Red Cross in
                        cooperation with an industry partner. The system connects people in need of general
                        assistance or with urgent medical emergencies with helping users around them. &mdash; with
                        <a class="link" href="https://www.professoren.tum.de/en/bruegge-bernd/">Prof. Br√ºgge</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="https://youtu.be/Hd8rgOIprww?t=496" target="blank">video
                                1</a>
                            <a class="resource-link" href="https://youtu.be/B3cZeolpBLQ?t=289" target="blank">video
                                2</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/nlp.png" alt="Projekt 03">
                    </div>
                    <span>
                        In the master's seminar Applied Deep Learning in Natural Language Processing at the chair for <a
                            class="link" href="http://politicaldatascience.blogspot.com">Political Data Science</a>, I
                        improved an
                        existing approach to generate word embeddings. I used this system to compare gender bias in
                        religious texts. &mdash; with <a class="link"
                            href="https://www.hfp.tum.de/en/professorships/associate-professor-for-political-data-science/">Prof.
                            Hegelich</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="../downloads/research/nlp_compressed.pdf"
                                target="blank">slides</a>
                            <a class="resource-link" href="https://github.com/axkoenig/word_embeddings"
                                target="blank">code</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/imperial.png" alt="Projekt 02">
                    </div>
                    <span>
                        During my research internship at the <a class="link"
                            href="http://www.imperial.ac.uk/mechatronics-in-medicine">Mechatronics in Medicine Lab</a>
                        at Imperial College London, I built a modular ROS platform to intuitively control a robotic rig
                        with a gesture
                        tracker. Visual feedback via a virtual reality headset allows for remote teleoperation of
                        the robot. I also used data from an RGB-D camera to play with ideas regarding autonomous
                        robotic grasping. The <a class="link" href="https://ro-man2021.org">IEEE RO-MAN</a> conference
                        2021 accepted
                        our <a class="link" href="../downloads/research/roman_paper.pdf">paper</a> on the
                        educational use of this platform. &mdash; with <a class="link"
                            href="https://www.imperial.ac.uk/people/r.secoli">Dr. Secoli</a> and <a class="link"
                            href="https://www.imperial.ac.uk/people/f.rodriguez">Prof. Rodriguez y Baena</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="../downloads/research/roman_paper.pdf"
                                target="blank">paper</a>
                            <a class="resource-link" href="https://ieeexplore.ieee.org/document/9515390"
                                target="blank">IEEE Xplore</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=fNvErR5eUpc"
                                target="blank">talk</a>
                            <a class="resource-link" href="https://github.com/axkoenig/leap_teleop"
                                target="blank">code</a>
                            <a class="resource-link" href="../downloads/research/handbook_compressed.pdf"
                                target="blank">handbook</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=RDbpd9d7U2k"
                                target="blank">demo</a>
                            <a class="resource-link"
                                href="https://www.imperial.ac.uk/mechatronics-in-medicine/research/human-robot-interaction-for-object-manipulation/"
                                target="blank">Blog</a>
                        </div>
                    </span>
                </div>
                <div class="row">
                    <div>
                        <img src="../images/research/bt.png" alt="Projekt 01">
                    </div>
                    <span>
                        For my bachelor thesis at the <a class="link"
                            href="http://campar.in.tum.de/Main/NarvisLabNew">NARVIS Lab</a>, I developed an
                        application for the Microsoft HoloLens to support orthopedic trauma surgeons with
                        intra-operative 3D visualizations of complex bone
                        fractures. I conducted a user study with four trauma surgeons to evaluate my work.
                        &mdash; with <a class="link" href="http://campar.in.tum.de/Main/UlrichEck">Dr. Eck</a> and <a
                            class="link" href="https://www.professoren.tum.de/en/navab-nassir/">Prof. Navab</a>
                        <div class="paper-resources">
                            <a class="resource-link" href="../downloads/research/bt_compressed.pdf"
                                target="blank">thesis</a>
                            <a class="resource-link" href="../downloads/research/bt_slides_compressed.pdf"
                                target="blank">slides</a>
                            <a class="resource-link" href="https://www.youtube.com/watch?v=WQMYF8R2ZdI"
                                target="blank">video</a>
                        </div>
                    </span>
                </div>
            </div>
        </div>
    </div>
</body>

</html>